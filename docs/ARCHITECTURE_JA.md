# 最適化フレームワークのアーキテクチャ概要

このドキュメントでは、教育用途およびプロトタイピング向けに設計された軽量な最適化フレームワークの全体構造と設計思想を説明します。本フレームワークは勾配降下法を中心に、目的関数の定義から学習ループ、コールバックを用いた拡張まで、一貫した流れで利用できることを目的としています。リポジトリ再編により、勾配降下モジュールは `topics/gradient_descent/` 以下に移動し、トピックごとに独立した構造を保ちながら共通のビルドシステムを共有しています。

## 1. 全体像

```
アプリケーション (topics/gradient_descent/examples)
    ↓
トレーナー (trainer)
    ↓
オプティマイザ (optim)
    ↓
モデル / ロス (model, loss)

各イテレーションでコールバック (callbacks) を実行
```

アプリケーションコードは `gd_train_minimize` を呼び出し、トレーナーが学習ループを管理します。トレーナーはモデルとロスを通じて目的関数値と勾配を計算し、オプティマイザの更新規則に従ってパラメータを更新します。各反復ではコールバックが実行され、ロギングや学習率スケジューリング、早期終了などを実現できます。

## 2. 主なモジュールと責務

### 2.1 型定義 (`topics/gradient_descent/include/gd/types.h`)

* `GD_Scalar` や `GD_Vector` といった基本的な数値型、配列ハンドルを定義します。
* 共通で利用する列挙型やステータスコードもここに集約されています。

### 2.2 モデル (`topics/gradient_descent/include/gd/model.h`, `topics/gradient_descent/src/model.c`)

* ユーザーが定義する目的関数 `f(x)` と勾配 `g(x)` のインターフェースを規定します。
* 勾配が未提供の場合のハンドリングはロスモジュールが担当します。

### 2.3 ロス (`topics/gradient_descent/include/gd/loss.h`, `topics/gradient_descent/src/loss.c`)

* モデルから目的関数値を取得し、必要に応じて有限差分による数値勾配を計算します。
* 数値微分のステップ幅はデフォルトで `1e-6` に設定されています。

### 2.4 オプティマイザ (`topics/gradient_descent/include/gd/optim.h`, `topics/gradient_descent/src/optim.c`)

* 勾配降下法のパラメータ更新を実装します。
* 学習率や許容誤差などのハイパーパラメータを `GD_OptimConfig` で管理します。

### 2.5 トレーナー (`topics/gradient_descent/include/gd/trainer.h`, `topics/gradient_descent/src/trainer.c`)

* 学習ループの中核として、以下の手順を繰り返します。
  1. 現在のパラメータで目的関数値を評価。
  2. 勾配を計算（モデルが提供する解析的勾配、または数値勾配）。
  3. コールバックをすべて実行。
  4. 無限大ノルムが許容誤差を下回ったか確認。
  5. オプティマイザに更新を指示してパラメータを更新。
* 最大反復回数や収束判定を管理します。

### 2.6 コールバック (`topics/gradient_descent/include/gd/callbacks.h`, `topics/gradient_descent/src/callbacks.c`)

* 任意の処理を各イテレーションで実行できるフック機構を提供します。
* 組み込みコールバックとして、標準出力へのログ表示や CSV への進捗記録などを実装しています。
* 学習率スケジューラや早期終了ロジックなど、追加の機能もコールバックで拡張可能です。

### 2.7 ユーティリティ (`topics/gradient_descent/src/utils.c`, `topics/gradient_descent/src/utils.h`)

* ベクトル操作や安全なファイル操作など、モジュール間で共有される補助関数を定義します。

## 3. ディレクトリ構成

```
topics/gradient_descent/
  include/gd/   … 公開ヘッダー群 (API)
  src/          … 実装ファイル
  examples/     … サンプルプログラムと補助スクリプト
scripts/run_gradient_descent.sh … ビルドとデモ実行用ヘルパー
```

サンプルとして `gd1d.c` と `gd2d.c` があり、1 変数および 2 変数の関数最小化の流れを示しています。各サンプルでは CSV ロガー（`scripts/run_gradient_descent.sh` から実行）を用いて学習過程をファイルに記録します。

## 4. トレーニングループのライフサイクル

トレーナーが 1 イテレーションで行う処理を以下にまとめます。

1. 現在のパラメータ `x` で `gd_loss_eval` により目的関数値 `f(x)` を計算。
2. `gd_loss_grad` を通じて勾配 `g(x)` を取得（解析的または数値的）。
3. コールバックスロットに登録された関数を順次実行。
4. `‖g(x)‖∞ < eps` を満たす場合は収束とみなし終了。
5. それ以外の場合、`gd_optim_step` で `x ← x − lr · g(x)` を計算し、次の反復へ進む。

## 5. コールバックによる拡張

コールバックは `GD_CallbackSlot` として登録し、以下のような用途に利用できます。

* 学習進捗の標準出力またはファイルへのロギング。
* 学習率の動的変更（スケジューラ）。
* 条件付きの早期終了（例: 損失が一定値を下回ったときに停止）。

組み込みの CSV ロガーコールバックは、トレーニング中の反復数、損失値、勾配ノルム、学習率などを行単位で CSV ファイルに追記し、長期的な解析や可視化を容易にします。

## 6. 拡張ポイントと今後の発展

* **新しいオプティマイザ**: モメンタム法や Adam などを `src/optim_*.c` として追加し、`GD_OptimConfig` に設定項目を拡張できます。
* **追加コールバック**: 例えば勾配クリッピング、可視化ツールとの連携などを目的に新しいコールバックを実装可能です。
* **制約条件への対応**: 投影法をコールバックで実装し、パラメータを所望の領域に制限することができます。

## 7. ビルドと実行

```
# 1D のデモをビルド・実行
scripts/run_gradient_descent.sh --example 1d

# 2D のデモをビルド・実行
scripts/run_gradient_descent.sh --example 2d
```

生成された CSV は `topics/gradient_descent/examples/outputs/` に配置され、トレーニング過程の記録を確認できます。

以上が勾配降下モジュールのアーキテクチャ概要です。ヘッダーファイルで公開された API を参照しながら、必要に応じて各モジュールを拡張し、自分の用途に合わせた最適化実験に活用してください。
